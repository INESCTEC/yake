ğŸ¯ PLANO DE OTIMIZAÃ‡Ã•ES PRIORITÃRIAS
=====================================

Baseado no profiling executado, aqui estÃ£o as otimizaÃ§Ãµes recomendadas
em ordem de prioridade (impacto vs esforÃ§o).

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ¥‡ PRIORIDADE 1: ComposedWord.__init__ (17% do tempo)

### Problema Identificado
- 10,050 chamadas durante processamento
- 0.043s total (17% do tempo)
- CriaÃ§Ã£o de objetos Ã© cara

### OtimizaÃ§Ã£o 1.1: Adicionar __slots__
**Impacto esperado:** 15-25% melhoria
**Dificuldade:** Baixa
**Tempo:** 15 minutos

```python
# yake/data/composed_word.py

class ComposedWord:
    __slots__ = (
        'surface_forms', 'terms', 'term_occur_set', 'tf',
        'cand', 'sentence_ids', 'unique_term', 'stopword_count',
        'max_term_occur', '_cached_hash'
    )
    
    def __init__(self, term_list, surface_forms=None):
        self.surface_forms = surface_forms or []
        self.terms = term_list
        # ... resto do cÃ³digo
```

**BenefÃ­cios:**
- âœ… Reduz uso de memÃ³ria (~40%)
- âœ… Acesso a atributos mais rÃ¡pido (~10-20%)
- âœ… Sem mudanÃ§a na API pÃºblica

**AtenÃ§Ã£o:**
- âš ï¸ NÃ£o permite adicionar atributos dinÃ¢micos
- âš ï¸ Precisa listar TODOS os atributos usados

---

### OtimizaÃ§Ã£o 1.2: Lazy Evaluation de Propriedades
**Impacto esperado:** 10-15% melhoria
**Dificuldade:** MÃ©dia
**Tempo:** 30 minutos

```python
class ComposedWord:
    __slots__ = (..., '_cached_surface_form', '_cached_unique_term')
    
    def __init__(self, term_list, surface_forms=None):
        self.terms = term_list
        self.surface_forms = surface_forms or []
        self._cached_surface_form = None
        self._cached_unique_term = None
    
    @property
    def surface_form(self):
        """Calcula apenas quando necessÃ¡rio"""
        if self._cached_surface_form is None:
            self._cached_surface_form = ' '.join(self.terms)
        return self._cached_surface_form
    
    @property
    def unique_term(self):
        """Calcula apenas quando necessÃ¡rio"""
        if self._cached_unique_term is None:
            self._cached_unique_term = '|'.join(sorted(set(self.terms)))
        return self._cached_unique_term
```

**BenefÃ­cios:**
- âœ… Evita cÃ¡lculos desnecessÃ¡rios
- âœ… Cache automÃ¡tico de valores computados
- âœ… CompatÃ­vel com __slots__

---

### OtimizaÃ§Ã£o 1.3: String Interning para Terms Comuns
**Impacto esperado:** 5-10% melhoria memÃ³ria
**Dificuldade:** Baixa
**Tempo:** 10 minutos

```python
# yake/data/core.py

class DataCore:
    def __init__(self, ...):
        self._term_cache = {}  # Cache de termos
    
    def _intern_term(self, term):
        """Reutiliza strings idÃªnticas"""
        if term not in self._term_cache:
            self._term_cache[term] = term
        return self._term_cache[term]
    
    def _generate_candidates(self, ...):
        # Usar termos internados
        terms = [self._intern_term(t) for t in candidate_terms]
        composed = ComposedWord(terms, ...)
```

**BenefÃ­cios:**
- âœ… Reduz duplicaÃ§Ã£o de strings
- âœ… Melhora performance de comparaÃ§Ãµes
- âœ… Menor uso de memÃ³ria

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ¥ˆ PRIORIDADE 2: get_tag() (15% do tempo)

### Problema Identificado
- 3,600 chamadas
- 0.040s total (15% do tempo)
- Muitas chamadas repetidas para mesmas palavras

### OtimizaÃ§Ã£o 2.1: LRU Cache
**Impacto esperado:** 40-60% melhoria nesta funÃ§Ã£o
**Dificuldade:** Muito Baixa
**Tempo:** 5 minutos

```python
# yake/data/utils.py
from functools import lru_cache

@lru_cache(maxsize=10000)
def get_tag(word):
    """
    Cache de tags para palavras jÃ¡ processadas.
    
    Nota: maxsize=10000 Ã© suficiente para a maioria dos textos.
    Para textos muito grandes (>1M palavras), considere aumentar.
    """
    # ... cÃ³digo existente
```

**BenefÃ­cios:**
- âœ… MUITO simples - apenas 1 linha!
- âœ… Grande impacto (~10-15% do tempo total)
- âœ… Automatic eviction de entradas antigas

**Alternativa para textos MUITO grandes:**
```python
# Cache customizado com limite de memÃ³ria
class TagCache:
    def __init__(self, max_size=50000):
        self._cache = {}
        self._max_size = max_size
    
    def get_tag(self, word):
        if word in self._cache:
            return self._cache[word]
        
        tag = _compute_tag(word)  # funÃ§Ã£o original
        
        # Limpar cache se muito grande
        if len(self._cache) >= self._max_size:
            # Remover 20% das entradas mais antigas
            remove_count = self._max_size // 5
            for key in list(self._cache.keys())[:remove_count]:
                del self._cache[key]
        
        self._cache[word] = tag
        return tag
```

---

### OtimizaÃ§Ã£o 2.2: Otimizar Regex Patterns
**Impacto esperado:** 5-10% melhoria
**Dificuldade:** MÃ©dia
**Tempo:** 20 minutos

```python
# yake/data/utils.py

# PrÃ©-compilar patterns (mover para nÃ­vel de mÃ³dulo)
ALPHA_PATTERN = re.compile(r'[a-zA-Z]')
DIGIT_PATTERN = re.compile(r'\d')

def get_tag(word):
    # Usar patterns prÃ©-compilados
    has_alpha = bool(ALPHA_PATTERN.search(word))
    has_digit = bool(DIGIT_PATTERN.search(word))
    
    # ... resto do cÃ³digo
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ¥‰ PRIORIDADE 3: add_or_update_composedword (13% do tempo)

### Problema Identificado
- 10,050 chamadas
- 0.034s total (13% do tempo)
- Lookups em dicionÃ¡rio de candidatos

### OtimizaÃ§Ã£o 3.1: Usar defaultdict
**Impacto esperado:** 5-10% melhoria
**Dificuldade:** Baixa
**Tempo:** 15 minutos

```python
# yake/data/core.py
from collections import defaultdict

class DataCore:
    def __init__(self, ...):
        # Ao invÃ©s de dict normal
        self._candidates = defaultdict(lambda: None)
    
    def add_or_update_composedword(self, composed_word):
        """VersÃ£o otimizada"""
        unique_term = composed_word.unique_term
        
        existing = self._candidates.get(unique_term)
        if existing is None:
            self._candidates[unique_term] = composed_word
        else:
            existing.update_cand(composed_word)
```

---

### OtimizaÃ§Ã£o 3.2: Batch Updates
**Impacto esperado:** 10-15% melhoria
**Dificuldade:** MÃ©dia/Alta
**Tempo:** 1 hora

```python
class DataCore:
    def _process_sentence(self, sentence, sentence_id):
        """Processa sentenÃ§a com batch updates"""
        # Coletar todas as atualizaÃ§Ãµes
        pending_updates = []
        
        for word_data in sentence:
            # ... processar palavra
            candidates = self._generate_candidates(...)
            pending_updates.extend(candidates)
        
        # Aplicar todas de uma vez
        self._batch_update_candidates(pending_updates)
    
    def _batch_update_candidates(self, candidates_list):
        """Atualiza mÃºltiplos candidatos de uma vez"""
        # Agrupar por unique_term
        grouped = defaultdict(list)
        for cand in candidates_list:
            grouped[cand.unique_term].append(cand)
        
        # Aplicar updates agrupados
        for unique_term, cands in grouped.items():
            if unique_term in self._candidates:
                self._candidates[unique_term].merge_batch(cands)
            else:
                self._candidates[unique_term] = cands[0]
                for cand in cands[1:]:
                    self._candidates[unique_term].update_cand(cand)
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ¯ PRIORIDADE 4: TokenizaÃ§Ã£o (24% do tempo)

### Problema Identificado
- 24% do tempo em bibliotecas externas (segtok)
- DifÃ­cil de otimizar diretamente

### OtimizaÃ§Ã£o 4.1: Avaliar Tokenizers Alternativos
**Impacto esperado:** 20-50% melhoria (se trocar)
**Dificuldade:** Alta
**Tempo:** 2-4 horas

```python
# yake/data/utils.py

# OpÃ§Ã£o 1: spaCy (mais rÃ¡pido, mas pesado)
def tokenize_sentences_spacy(text, language='en'):
    import spacy
    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])
    doc = nlp(text)
    return [[token.text for token in sent] for sent in doc.sents]

# OpÃ§Ã£o 2: Stanza (bom compromisso)
def tokenize_sentences_stanza(text, language='en'):
    import stanza
    nlp = stanza.Pipeline(language, processors='tokenize')
    doc = nlp(text)
    return [[token.text for token in sent.tokens] for sent in doc.sentences]

# OpÃ§Ã£o 3: Regex simples (mais rÃ¡pido, menos preciso)
def tokenize_sentences_simple(text):
    import re
    # Split por pontuaÃ§Ã£o de fim de sentenÃ§a
    sentences = re.split(r'[.!?]+', text)
    return [re.findall(r'\b\w+\b', sent) for sent in sentences if sent.strip()]
```

**RecomendaÃ§Ã£o:**
- âš ï¸ Trocar tokenizer Ã© arriscado - afeta resultados
- âœ… Criar flag opcional para escolher tokenizer
- âœ… Benchmarkar com cada opÃ§Ã£o
- âœ… Validar que keywords extraÃ­das sÃ£o similares

---

### OtimizaÃ§Ã£o 4.2: Cache de TokenizaÃ§Ã£o
**Impacto esperado:** VariÃ¡vel (depende de duplicaÃ§Ã£o)
**Dificuldade:** Baixa
**Tempo:** 15 minutos

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def _tokenize_sentence_cached(sentence):
    """Cache para sentenÃ§as idÃªnticas"""
    return word_tokenizer(sentence)

def tokenize_sentences(text, language='en'):
    sentences = split_sentences(text)
    return [_tokenize_sentence_cached(s) for s in sentences]
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸ“Š RESUMO E ROADMAP

### Quick Wins (< 30 min, alto impacto)
1. âœ… **@lru_cache em get_tag()** â†’ ~10-15% ganho
2. âœ… **__slots__ em ComposedWord** â†’ ~15-20% ganho
3. âœ… **String interning** â†’ ~5-10% ganho memÃ³ria

### Ganhos MÃ©dios (30-60 min)
4. âœ… **Lazy evaluation** â†’ ~10-15% ganho
5. âœ… **defaultdict para candidatos** â†’ ~5-10% ganho
6. âœ… **PrÃ©-compilar regex** â†’ ~5% ganho

### Projetos Maiores (> 1 hora)
7. âš ï¸ **Batch updates** â†’ ~10-15% ganho (complexo)
8. âš ï¸ **Tokenizer alternativo** â†’ ~20-50% ganho (arriscado)

### Ganho Total Esperado
- **Quick wins:** ~30-45% melhoria
- **Ganhos mÃ©dios:** +15-25% adicional
- **Projetos maiores:** +20-50% adicional (se bem sucedidos)
- **TOTAL POTENCIAL:** 50-100% melhoria

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## ğŸš€ ORDEM DE IMPLEMENTAÃ‡ÃƒO RECOMENDADA

### Fase 1: Quick Wins (implementar hoje!)
```bash
1. Adicionar @lru_cache a get_tag()
2. Adicionar __slots__ a ComposedWord
3. Implementar string interning bÃ¡sico

# Validar
python benchmark_compare.py
```

### Fase 2: Refinamentos (prÃ³xima semana)
```bash
4. Lazy evaluation de propriedades
5. defaultdict para candidatos
6. PrÃ©-compilar regex patterns

# Validar
python benchmark_compare.py
```

### Fase 3: ExploratÃ³ria (se necessÃ¡rio)
```bash
7. Experimentar tokenizers alternativos
8. Implementar batch updates (se crÃ­tico)

# Validar extensivamente
python benchmark_compare.py
python tests/test_yake.py  # garantir mesmos resultados
```

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

## âš ï¸ AVISOS IMPORTANTES

### O que NÃƒO fazer:
âŒ NÃ£o otimizar `_update_cooccurrence` - apenas 9% e cÃ³digo complexo
âŒ NÃ£o mexer em `SingleWord.update_h()` - apenas 1.5% do tempo
âŒ NÃ£o otimizar prematuramente outras funÃ§Ãµes <5%

### ValidaÃ§Ã£o ObrigatÃ³ria:
âœ… Executar benchmark antes e depois
âœ… Rodar testes unitÃ¡rios
âœ… Comparar keywords extraÃ­das (devem ser idÃªnticas)
âœ… Testar com textos de diferentes tamanhos

### Monitoramento:
ğŸ“Š Use `benchmark_compare.py` apÃ³s cada otimizaÃ§Ã£o
ğŸ“Š Documente ganhos reais vs esperados
ğŸ“Š Se ganho < 5%, considere reverter (nÃ£o vale complexidade)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Quer que eu implemente alguma dessas otimizaÃ§Ãµes agora? Recomendo comeÃ§ar
pelas Quick Wins - sÃ£o simples, seguras e tÃªm alto impacto! ğŸš€
